{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1be33f",
   "metadata": {},
   "source": [
    "# Lab 7 Report: \n",
    "## Stock Prediction AI with Encoder-Decoder RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02d733",
   "metadata": {},
   "source": [
    "### Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a175cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image # For displaying images in colab jupyter cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('lab7_exercise.png', width = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn plot styling\n",
    "sns.set(style = 'white', font_scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b4738",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock datasets\n",
    "# Pick one of three to train your model \n",
    "# Use 'closing price' column for training and testing\n",
    "\n",
    "tesla = pd.read_csv('TSLA.csv') \n",
    "tesla_np = tesla.to_numpy()\n",
    "\n",
    "google = pd.read_csv('GOOGL.csv') \n",
    "google_np = google.to_numpy()\n",
    "\n",
    "dji = pd.read_csv('DJI.csv') \n",
    "dji_np = dji.to_numpy()\n",
    "print(tesla_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize your data and select training dataset (all the days except for last 100 days)\n",
    "training_raw = tesla_np[:,5]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "training_raw = scaler.fit_transform(training_raw.reshape(-1,1))\n",
    "print(training_raw.shape)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your encoder input sequence length, decoder output sequence length and testing sequence length\n",
    "# Construct train_input_seqs and train_output_seqs according to \n",
    "# encoder input sequence length and decoder output sequence length similar to example task\n",
    "encoder_inputseq_len = 7\n",
    "decoder_outputseq_len = 3\n",
    "testing_sequence_len = 50\n",
    "\n",
    "num_samples = training_raw.shape[0] - encoder_inputseq_len - decoder_outputseq_len + 1\n",
    "\n",
    "train_input_seqs = np.zeros((num_samples, encoder_inputseq_len, 1))\n",
    "train_output_seqs = np.zeros((num_samples, decoder_outputseq_len, 1))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_samples):\n",
    "    train_input_seqs[i] = training_raw[i:i+encoder_inputseq_len]\n",
    "    train_output_seqs[i] = training_raw[i+encoder_inputseq_len:i+encoder_inputseq_len+decoder_outputseq_len]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure train_input_seqs and train_output_seqs have correct dimensions as expected\n",
    "# (sample size, sequence length, # of features / timestep)\n",
    "\n",
    "print(\"Encoder Training Inputs Shape: \", train_input_seqs.shape)\n",
    "print(\"Decoder Training Outputs Shape: \", train_output_seqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda0488",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53860c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \n",
    "        out, hidden = self.lstm(input_seq, hidden_state)\n",
    "        \n",
    "        return out, hidden     \n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)# YOUR CODE HERE  \n",
    "        self.fc_decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_seq, encoder_hidden_states):\n",
    "        out, hidden = self.lstm(input_seq, encoder_hidden_states)\n",
    "        output = self.fc_decoder(out)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "class Encoder_Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, decoder_output_size, num_layers):\n",
    "\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.Decoder = Decoder(input_size=input_size, hidden_size=hidden_size, output_size=decoder_output_size, num_layers=num_layers)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac20f78",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "Encoder_Decoder_RNN = Encoder_Decoder(input_size=1, hidden_size=20, decoder_output_size=1, num_layers=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "batchsize = 10\n",
    "num_features = 1\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Encoder_Decoder_RNN.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fba0e",
   "metadata": {},
   "source": [
    "## Identify Tracked Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403ae44",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data into torch tensors\n",
    "\n",
    "train_input_seqs = torch.from_numpy(train_input_seqs).float()\n",
    "train_output_seqs = torch.from_numpy(train_output_seqs).float()\n",
    "# Split training data into mini-batches\n",
    "train_batches_features = torch.split(train_input_seqs, batchsize)[:-1]\n",
    "train_batches_targets = torch.split(train_output_seqs, batchsize)[:-1]\n",
    "\n",
    "batch_split_num = len(train_batches_features)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compute total number of mini-batches in training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed7297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#problem seems to be with output and not input?\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    \n",
    "    for k in range(batch_split_num): \n",
    "        \n",
    "        hidden_state = None\n",
    "        decoder_output_seq = torch.zeros(batchsize, decoder_outputseq_len, 1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encoder_output, encoder_hidden = Encoder_Decoder_RNN.Encoder(train_batches_features[k], hidden_state)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        \n",
    "        print(encoder_output)\n",
    "        decoder_input = train_batches_features[k][:, -1, :]\n",
    "        \n",
    "        decoder_input = torch.unsqueeze(decoder_input, 2)\n",
    "        \n",
    "        for t in range(decoder_outputseq_len):\n",
    "            \n",
    "            decoder_output, decoder_hidden = Encoder_Decoder_RNN.Decoder(decoder_input, decoder_hidden)\n",
    "           \n",
    "            decoder_output_seq[:, t, :] = torch.squeeze(decoder_output, 2)\n",
    "            \n",
    "            decoder_input = train_batches_targets[k][:, t, :]\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 2)\n",
    "\n",
    "        loss = loss_func(torch.squeeze(decoder_output_seq), torch.squeeze(train_batches_targets[k]))\n",
    "        \n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Averaged Training Loss for Epoch \", epoch,\": \", np.mean(train_loss_list[-batch_split_num:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b49cc5",
   "metadata": {},
   "source": [
    "## Visualize & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72efa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "plt.plot(np.convolve(train_loss_list, np.ones(100), 'valid') / 100, \n",
    "         linewidth = 3, label = 'Rolling Averaged Training Loss')\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your testing sequence\n",
    "\n",
    "test_input_seq = training_raw[-200:-150]\n",
    "print(type(test_input_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the testing sequence\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(test_input_seq, linewidth = 3)\n",
    "plt.title('Test Sequence')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4eceb",
   "metadata": {},
   "source": [
    "### Generate signal predictions for testing sequence with trained Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00988cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USE TEACHER FORCING METHOD WHEN GENERATING OUTPUTS FROM DECODER\n",
    "# See slide 42 of Lab 5 or Lab 5 part 2 video to recap the concept of teacher forcing method\n",
    "# When generating decoder outputs, make sure each input to decoder at timestep t has the shape (1,1,1)\n",
    "# i.e., num_samples = 1, sequence_len = 1, num_features = 1 \n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "test_input_seq = torch.from_numpy(test_input_seq).float()\n",
    "\n",
    "decoder_output_seq = torch.zeros(50, num_features)\n",
    "\n",
    "decoder_output_seq[:encoder_input_length] = test_input_seq[:encoder_input_length]\n",
    "\n",
    "\n",
    "pred_start_ind = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    while pred_start_ind + encoder_input_length + decoder_output_length < test_sequence_length:\n",
    "        \n",
    "        hidden_state = None\n",
    "        \n",
    "        input_test_seq = decoder_output_seq[pred_start_ind:pred_start_ind + encoder_input_length]\n",
    "        input_test_seq = torch.unsqueeze(input_test_seq, 0)\n",
    "        \n",
    "        encoder_output, encoder_hidden = Encoder_Decoder_RNN.Encoder(input_test_seq, hidden_state)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoder_input = input_test_seq[:, -1, :]\n",
    "        decoder_input = torch.unsqueeze(decoder_input, 2)\n",
    "        \n",
    "        for t in range(decoder_output_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = Encoder_Decoder_RNN.Decoder(decoder_input, decoder_hidden)\n",
    "            decoder_output_seq[pred_start_ind + encoder_input_length + t] = torch.squeeze(decoder_output)\n",
    "            decoder_input = decoder_output\n",
    "        \n",
    "        pred_start_ind += decoder_output_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbccae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predicted stock sequence vs the ground truth\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.plot(test_input_seq, linewidth = 3, label = 'GroundTruth')\n",
    "plt.plot(decoder_output_seq, linewidth = 3, label = 'RNN Predicted')\n",
    "plt.title('RNN Predicted vs GroundTruth')\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8858706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MSE error between test_input_seq and decoder_output_seq and print the value as Test MSE Error\n",
    "\n",
    "print(loss_func(decoder_output_seq, test_input_seq).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43513500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
