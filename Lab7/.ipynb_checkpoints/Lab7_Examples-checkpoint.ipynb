{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1be33f",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> LAB 7: </span>\n",
    "# <span style='color:blue'> ENCODER-DECODER RNN ARCHITECTURE </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b31b20",
   "metadata": {},
   "source": [
    "## <span style='color:red'> Part 2: Encoder-Decoder in PyTorch </span>\n",
    "## <span style='color:red'> Signal Prediction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn plot styling\n",
    "sns.set(style = 'white', font_scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b4738",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_signal(datapoints_num, tf):\n",
    "    \n",
    "    # Create time domain vector according to datapoints_num\n",
    "    t = np.linspace(0., tf, datapoints_num)\n",
    "    \n",
    "    # Create noisy wave (sin + cos + gaussian noise)\n",
    "    y = np.sin(2. * t) + 0.5 * np.cos(t) + np.random.normal(0., 0.2, datapoints_num)\n",
    "    \n",
    "    # Reshape y in vertical orientation, i.e., (datapoints_num, 1) shape\n",
    "    return y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20110e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_seqs(y, encoder_inputseq_len, decoder_outputseq_len, stride = 1, num_features = 1):\n",
    "  \n",
    "    L = y.shape[0] # Length of y\n",
    "    \n",
    "    # Calculate how many input/target sequences there will be based on the parameters and stride\n",
    "    num_samples = (L - encoder_inputseq_len - decoder_outputseq_len) // stride + 1\n",
    "    \n",
    "    # Numpy zeros arrray to contain the input/target sequences\n",
    "    # Note that they should be in (num_samples, seq_len, num_features/time step) format\n",
    "    train_input_seqs = np.zeros([num_samples, encoder_inputseq_len, num_features])\n",
    "    train_output_seqs = np.zeros([num_samples, decoder_outputseq_len, num_features])    \n",
    "    \n",
    "    # Iteratively fill in train_input_seqs and train_output_seqs\n",
    "    # See slide 17 of lab 7 to get an idea of how input_seqs and output_seqs look like\n",
    "    for ff in np.arange(num_features):\n",
    "        \n",
    "        for ii in np.arange(num_samples):\n",
    "            \n",
    "            start_x = stride * ii\n",
    "            end_x = start_x + encoder_inputseq_len\n",
    "            train_input_seqs[ii, :, ff] = y[start_x:end_x, ff]\n",
    "\n",
    "            start_y = stride * ii + encoder_inputseq_len\n",
    "            end_y = start_y + decoder_outputseq_len \n",
    "            train_output_seqs[ii, :, ff] = y[start_y:end_y, ff]\n",
    "\n",
    "    return train_input_seqs, train_output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder/decoder sequence lengths and testing sequencce length\n",
    "encoder_inputseq_len = 5 \n",
    "decoder_outputseq_len = 2 \n",
    "testing_sequence_len = 50\n",
    "\n",
    "# Generate noisy signal\n",
    "y = generate_noisy_signal(datapoints_num = 2000, tf = 80 * np.pi)\n",
    "\n",
    "# Using the all the datapoints except for testing sequence (last 50 datapoints) for training\n",
    "y_train = y[:-testing_sequence_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42000907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the last 300 datapoints of our training data\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.plot(y_train[-300:], linewidth = 3, color = 'grey')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate encoder input seqs and decoder output seqs\n",
    "train_input_seqs, train_output_seqs = generate_input_output_seqs(y = y_train,\n",
    "                                                                 encoder_inputseq_len = encoder_inputseq_len,\n",
    "                                                                 decoder_outputseq_len = decoder_outputseq_len,\n",
    "                                                                 stride = 1,\n",
    "                                                                 num_features = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of encoder input seqs and decoder output seqs\n",
    "print(\"Encoder Training Inputs Shape: \", train_input_seqs.shape)\n",
    "print(\"Decoder Training Outputs Shape: \", train_output_seqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda0488",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53860c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Using LSTM for Encoder with batch_first = True\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size = input_size, hidden_size = hidden_size, \n",
    "                                  num_layers = num_layers, \n",
    "                                  batch_first = True)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(input_seq, hidden_state)\n",
    "        \n",
    "        return lstm_out, hidden     \n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Using LSTM for Decoder with batch_first = True\n",
    "        # fc_decoder for converting hidden states -> single number\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size = input_size, hidden_size = hidden_size, \n",
    "                                  num_layers = num_layers,\n",
    "                                  batch_first = True)\n",
    "        \n",
    "        self.fc_decoder = torch.nn.Linear(hidden_size, output_size)     \n",
    "\n",
    "    def forward(self, input_seq, encoder_hidden_states):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(input_seq, encoder_hidden_states)\n",
    "        output = self.fc_decoder(lstm_out)     \n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "class Encoder_Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, decoder_output_size, num_layers):\n",
    "        \n",
    "        # Combine Encoder and Decoder classes into one\n",
    "\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(input_size = input_size, hidden_size = hidden_size, \n",
    "                               num_layers = num_layers)\n",
    "        \n",
    "        self.Decoder = Decoder(input_size = input_size, hidden_size = hidden_size, \n",
    "                               output_size = decoder_output_size, num_layers = num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac20f78",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7badc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "torch.manual_seed(55)\n",
    "\n",
    "# Using input_size = 1 (# of features to be fed to RNN per timestep)\n",
    "# Using decoder_output_size = 1 (# of features to be output by Decoder RNN per timestep)\n",
    "Encoder_Decoder_RNN = Encoder_Decoder(input_size = 1, hidden_size = 15, \n",
    "                                      decoder_output_size = 1, num_layers = 1)\n",
    "\n",
    "# Define learning rate + epochs\n",
    "learning_rate = 0.01         \n",
    "epochs = 50\n",
    "\n",
    "# Define batch size and num_features/timestep (this is simply the last dimension of train_output_seqs)\n",
    "batchsize = 5\n",
    "num_features = train_output_seqs.shape[2]\n",
    "\n",
    "# Define loss function/optimizer\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Encoder_Decoder_RNN.parameters(), lr=learning_rate)\n",
    "\n",
    "Encoder_Decoder_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fba0e",
   "metadata": {},
   "source": [
    "## Identify Tracked Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Python list to keep track of training loss\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403ae44",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training dataset into torch tensors\n",
    "train_input_seqs = torch.from_numpy(train_input_seqs).float()\n",
    "train_output_seqs = torch.from_numpy(train_output_seqs).float()\n",
    "\n",
    "# Split the training dataset to mini-batches\n",
    "# Skipping the last mini-batch since its size can be smaller than the set batchsize\n",
    "train_batches_features = torch.split(train_input_seqs, batchsize)[:-1]\n",
    "train_batches_targets = torch.split(train_output_seqs, batchsize)[:-1]\n",
    "\n",
    "# Total number of mini-batches in the training set\n",
    "batch_split_num = len(train_batches_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed7297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs): # For each epoch\n",
    "    \n",
    "    for k in range(batch_split_num): # For each mini_batch\n",
    "        \n",
    "        # initialize hidden states to Encoder\n",
    "        hidden_state = None\n",
    "        \n",
    "        # initialize empty torch tensor array to store decoder output sequence\n",
    "        decoder_output_seq = torch.zeros(batchsize, decoder_outputseq_len, num_features)\n",
    "        \n",
    "        # empty gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed k-th mini-batch for encoder input sequences to encoder with hidden state\n",
    "        encoder_output, encoder_hidden = Encoder_Decoder_RNN.Encoder(train_batches_features[k], hidden_state)\n",
    "        # Re-define the resulting encoder hidden states as input hidden states to decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # Initial input to decoder is last timestep feature from the encoder input sequence\n",
    "        decoder_input = train_batches_features[k][:, -1, :]\n",
    "        # The extracted feature is 2D so need to add additional 3rd dimension \n",
    "        # to conform to (sample size, seq_len, # of features)\n",
    "        decoder_input = torch.unsqueeze(decoder_input, 2)\n",
    "        \n",
    "        # Populating the decoder output sequence\n",
    "        for t in range(decoder_outputseq_len): # for each timestep in output sequence\n",
    "            \n",
    "            # Feed in the decoder_input and decoder_hidden to Decoder, get new output and hidden states\n",
    "            decoder_output, decoder_hidden = Encoder_Decoder_RNN.Decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # Populate the corresponding timestep in decoder output sequence \n",
    "            decoder_output_seq[:, t, :] = torch.squeeze(decoder_output, 2)\n",
    "            \n",
    "            # We are using teacher forcing so using the groundtruth training target as the next input        \n",
    "            decoder_input = train_batches_targets[k][:, t, :]\n",
    "            \n",
    "            # The extracted feature is 2D so need to add additional 3rd dimension \n",
    "            # to conform to (sample size, seq_len, # of features)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 2)\n",
    "        \n",
    "        # Compare the predicted decoder output sequence aginast the target sequence to compute the MSE loss\n",
    "        loss = loss_func(torch.squeeze(decoder_output_seq), torch.squeeze(train_batches_targets[k]))\n",
    "        \n",
    "        # Save the loss\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the RNN\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Averaged Training Loss for Epoch \", epoch,\": \", np.mean(train_loss_list[-batch_split_num:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b49cc5",
   "metadata": {},
   "source": [
    "## Visualize & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72efa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 7))\n",
    "\n",
    "plt.plot(np.convolve(train_loss_list, np.ones(100), 'valid') / 100, \n",
    "         linewidth = 3, label = 'Rolling Averaged Training Loss')\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test sequence (last 50 datapoints in our case)\n",
    "test_input_seq = y[-testing_sequence_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our test sequence\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(test_input_seq, linewidth = 3)\n",
    "plt.title('Test Sequence')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8d946",
   "metadata": {},
   "source": [
    "### Generate signal predictions for testing sequence with trained Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00988cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test sequence to tensor\n",
    "test_input_seq = torch.from_numpy(test_input_seq).float()\n",
    "\n",
    "# initialize empty torch tensor array to store decoder output sequence\n",
    "# This should be the same size as the test sequence\n",
    "decoder_output_seq = torch.zeros(testing_sequence_len, num_features)\n",
    "\n",
    "# First n-datapoints in decoder output sequence = First n-datapoints in ground truth test sequence\n",
    "# n = encoder_input_seq_len\n",
    "decoder_output_seq[:encoder_inputseq_len] = test_input_seq[:encoder_inputseq_len]\n",
    "\n",
    "# Initialize index for prediction\n",
    "pred_start_ind = 0\n",
    "\n",
    "# Activate no_grad() since we aren't performing backprop\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Loop continues until the RNN prediction reaches the end of the testing sequence length\n",
    "    while pred_start_ind + encoder_inputseq_len + decoder_outputseq_len < testing_sequence_len:\n",
    "        \n",
    "        # initialize hidden state for encoder\n",
    "        hidden_state = None\n",
    "        \n",
    "        # Define the input to encoder\n",
    "        input_test_seq = decoder_output_seq[pred_start_ind:pred_start_ind + encoder_inputseq_len]\n",
    "        # Add dimension to first dimension to keep the input (sample_size, seq_len, # of features/timestep)\n",
    "        input_test_seq = torch.unsqueeze(input_test_seq, 0)\n",
    "        \n",
    "        # Feed the input to encoder and set resulting hidden states as input hidden states to decoder\n",
    "        encoder_output, encoder_hidden = Encoder_Decoder_RNN.Encoder(input_test_seq, hidden_state)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # Initial input to decoder is last timestep feature from the encoder input sequence \n",
    "        decoder_input = input_test_seq[:, -1, :]\n",
    "        # Add dimension to keep the input (sample_size, seq_len, # of features/timestep)\n",
    "        decoder_input = torch.unsqueeze(decoder_input, 2)\n",
    "        \n",
    "        # Populate decoder output sequence\n",
    "        for t in range(decoder_outputseq_len):\n",
    "            \n",
    "            # Generate new output for timestep t\n",
    "            decoder_output, decoder_hidden = Encoder_Decoder_RNN.Decoder(decoder_input, decoder_hidden)\n",
    "            # Populate the corresponding timestep in decoder output sequence\n",
    "            decoder_output_seq[pred_start_ind + encoder_inputseq_len + t] = torch.squeeze(decoder_output)\n",
    "            # Use the output of the decoder as new input for the next timestep\n",
    "            decoder_input = decoder_output\n",
    "        \n",
    "        # Update pred_start_ind\n",
    "        pred_start_ind += decoder_outputseq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbccae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the RNN prediction (decoder output sequence) vs the ground truth sequence\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.plot(test_input_seq, linewidth = 3, label = 'GroundTruth')\n",
    "plt.plot(decoder_output_seq, linewidth = 3, label = 'RNN Predicted')\n",
    "plt.title('RNN Predicted vs GroundTruth')\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
